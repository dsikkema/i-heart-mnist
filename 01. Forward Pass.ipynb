{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fde7d5-0f32-4347-9f1a-7e539d204563",
   "metadata": {},
   "source": [
    "In this notebook we'll make the basic forward pass and training loop for a fully connected network:\n",
    "- Inference logic, doing matrix multiplication to multiply each weight of each neuron by the associated value of the layer before it, and add bias, then do relu activation\n",
    "- Softmax function to turn final output into a representation of the probability distribution over predictions.\n",
    "- Cross Entropy Loss function which will be the objective we're trying to minimize\n",
    "- Utilizing pytorch's backpropagation to get the gradient, but then updating parameters manually with it and the learning rate\n",
    "- Iterating over epochs, and inside of epochs iterating over randomized minibatches, so that we're doing *stochastic* gradient descent.\n",
    "- **(Skip to the end for this) pick a random number out of the training set and show what the prediction is for the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b682b1f5-541b-48e9-b8ab-1de4d15ceac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40979f1-4ac3-4ac4-9756-0188ac4c01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b3a19e-fef5-41c5-83be-0af4c4077679",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "\n",
    "if not path_gz.exists():\n",
    "    urlretrieve(MNIST_URL, path_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b108a17e-a5f8-49e8-86fa-b8e79a7ab024",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(path_gz, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    \n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbed8c79-abbf-4ba2-9c06-340dabf160ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE=x_train.shape[0]\n",
    "BATCH_SIZE=10000\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9737beae-b8f7-4d3a-ad66-7453e2e58da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size):\n",
    "    '''\n",
    "    Xavier Weight Initialization\n",
    "    strategy for initializing values of weights meant to prevent gradient vanishing or explosion.\n",
    "    '''\n",
    "    bound = math.sqrt(1.0 / size[0])\n",
    "    return torch.randn(size).uniform_(-bound, bound).requires_grad_()\n",
    "\n",
    "def softmax(x):\n",
    "    e=x.exp()\n",
    "    return e / e.sum(-1,keepdim=True)\n",
    "\n",
    "def CrossEntropyLoss(p,y):\n",
    "    losst = -(p[range(p.shape[0]),y].log())\n",
    "    result = losst.mean()\n",
    "    return result\n",
    "\n",
    "class HomebrewModel:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        784: number of input features (pixels in 28x28 image)\n",
    "        200: number of feaures in the hidden layer (can be arbitrarily chosen)\n",
    "        '''\n",
    "        self.weights1 = init_params((784, 200)) # \n",
    "        self.bias1    = init_params((200, ))\n",
    "        '''\n",
    "        200: match the output coming from features of hidden layer\n",
    "        10: 10 possible output classifications (corresponding to the digits 0-9)\n",
    "        '''\n",
    "        self.weights2 = init_params((200, 10))\n",
    "        self.bias2    = init_params((10,))\n",
    "        \n",
    "    def predict(self,x):\n",
    "        # For each feature of the hidden layer, sum the products of each input feature with a corresponding weight and add a bias\n",
    "        x = (x@self.weights1 + self.bias1)\n",
    "        \n",
    "        # This is relu aka rectified linear unit activation\n",
    "        x = x.max(tensor(0.0))\n",
    "        \n",
    "        x = (x@self.weights2 + self.bias2)\n",
    "        \n",
    "        return softmax(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights1, self.bias1, self.weights2, self.bias2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79c5a64-79e6-460a-a880-b4c419cb2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "epoch goes through entire training set\n",
    "'''\n",
    "def train_epoch(model):\n",
    "    rand_indexes=torch.randperm(TRAIN_SIZE)\n",
    "    '''\n",
    "    each batch is a random subset of the training set, loop over batches to cover all training data\n",
    "    '''\n",
    "    for i in range(0, TRAIN_SIZE, BATCH_SIZE):\n",
    "        # pull the random batch\n",
    "        idxs = rand_indexes[i : i + BATCH_SIZE]\n",
    "        xb = x_train[idxs]\n",
    "        yb = y_train[idxs]\n",
    "        train_batch(model, xb, yb)\n",
    "\n",
    "                \n",
    "    # at the end of epoch, check model performance against validation data\n",
    "    with torch.no_grad():\n",
    "        valid_preds = model.predict(x_valid)\n",
    "        validation_acc = batch_accuracy(valid_preds, y_valid)\n",
    "        validation_loss = CrossEntropyLoss(valid_preds, y_valid)\n",
    "        print(f'valid/acc: {validation_acc}\\t valid/loss: {validation_loss}')\n",
    "\n",
    "def train_batch(model, xb, yb):\n",
    "    preds = model.predict(xb)\n",
    "    loss = CrossEntropyLoss(preds, yb)\n",
    "\n",
    "    # compute gradients: done by pytorch, see next notebook for backpropagation demonstration\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # update parameters to descend the gradient\n",
    "        for p in model.parameters():\n",
    "            p -= LEARNING_RATE * p.grad\n",
    "            p.grad.zero_()   \n",
    "    \n",
    "def batch_accuracy(preds_b, yb):\n",
    "    correct = (preds_b.argmax(1)) == yb\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93158cfc-ecaa-4732-81d4-69a9d986cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid/acc: 0.2662000060081482\t valid/loss: 2.196192979812622\n",
      "valid/acc: 0.5465999841690063\t valid/loss: 2.0844383239746094\n",
      "valid/acc: 0.6901000142097473\t valid/loss: 1.9524739980697632\n",
      "valid/acc: 0.7348999977111816\t valid/loss: 1.7984671592712402\n",
      "valid/acc: 0.7537000179290771\t valid/loss: 1.6301344633102417\n",
      "valid/acc: 0.7685999870300293\t valid/loss: 1.4609266519546509\n",
      "valid/acc: 0.7825000286102295\t valid/loss: 1.304023265838623\n",
      "valid/acc: 0.7993999719619751\t valid/loss: 1.1670347452163696\n",
      "valid/acc: 0.8137000203132629\t valid/loss: 1.0517596006393433\n",
      "valid/acc: 0.8274999856948853\t valid/loss: 0.9565532207489014\n",
      "valid/acc: 0.8364999890327454\t valid/loss: 0.8780364990234375\n",
      "valid/acc: 0.8454999923706055\t valid/loss: 0.8130989670753479\n",
      "valid/acc: 0.8508999943733215\t valid/loss: 0.7588340044021606\n",
      "valid/acc: 0.8554999828338623\t valid/loss: 0.7132889032363892\n",
      "valid/acc: 0.8611000180244446\t valid/loss: 0.6744224429130554\n",
      "valid/acc: 0.8644999861717224\t valid/loss: 0.6411890387535095\n",
      "valid/acc: 0.8686000108718872\t valid/loss: 0.612434446811676\n",
      "valid/acc: 0.8715000152587891\t valid/loss: 0.5871390700340271\n",
      "valid/acc: 0.8741000294685364\t valid/loss: 0.5652276277542114\n",
      "valid/acc: 0.8769000172615051\t valid/loss: 0.5456036925315857\n",
      "valid/acc: 0.8787000179290771\t valid/loss: 0.5279653072357178\n",
      "valid/acc: 0.8805999755859375\t valid/loss: 0.5124236345291138\n",
      "valid/acc: 0.8840000033378601\t valid/loss: 0.4984511137008667\n",
      "valid/acc: 0.8859000205993652\t valid/loss: 0.48595380783081055\n",
      "valid/acc: 0.8877999782562256\t valid/loss: 0.4745343327522278\n"
     ]
    }
   ],
   "source": [
    "model = HomebrewModel()\n",
    "for epoch in range(25):\n",
    "    train_epoch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc009a-0df2-4313-98ca-964ebdb3498b",
   "metadata": {},
   "source": [
    "# Pick a random number and try\n",
    "After running above cells, run cell below as many times as you please to pick a random number from the training set, see it, and see the prediction made for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc5130e-0e91-4df2-8c89-86a49f8c7f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n",
      "Confidence: 25.4%\n",
      "Label: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77bb052490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=torch.randint(0,TRAIN_SIZE-1, (1,)).item()\n",
    "preds = model.predict(x_train[i])\n",
    "prediction = preds.argmax()\n",
    "confidence = preds[prediction]\n",
    "print(f'Prediction: {prediction}\\nConfidence: {confidence:.1%}\\nLabel: {y_train[i]}')\n",
    "plt.imshow(x_train[i].view(28, 28) * -1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
